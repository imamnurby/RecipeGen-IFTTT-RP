import os
cwd = os.getcwd()

config_training={
    # ***** Dataset setting ***** 
    ## general
    "is_clean": None,
    "data_num":-1,                       # -1 for all data, otherwise specify the split

    ## training
    "do_train":True,
    "train_batch_size":8,
    "training_datapath":f"{os.path.join(cwd, 'dataset/ready-train-chen-only/train-chen.pkl')}",
    ## validation
    "do_validation":True,
    "val_batch_size":8,
    "validation_datapath":f"{os.path.join(cwd, 'dataset/ready-train-val-clean/validation.pkl')}",
    
    # ***** Training setting ***** 
    "n_cpu":None,                       # will be filled automatically
    "n_gpu":1,
    "seed":4321,
    "save_checkpoint":True,
    "num_train_epochs":50,
    "start_epoch":0,
    "eval_every_n_epoch":2,             
    "patience":3,                       # patience for early stopping
    
    # ***** model setting ***** 
    ## bert, roberta, codebert, codet5
    "model_type":"roberta",
    ## path to the model folder to load checkpoint if exist, it will be filled automatically if None                
    "model_name": None,                     
    ## if true, then load pretrained weight from huggingface
    "load_from_repo":True,

    # ***** hyperparams setting ***** 
    "num_warmup_steps":1000,            # optimizer setting
    "learning_rate":5e-6,               # optimizer setting
    "weight_decay":0.0,                 # optimizer setting
    "adam_epsilon":1e-8,                # optimizer setting
    "local_rank":-1,                    # for distributed training: local_rank
    "early_stopping":False,             # set to False by default
    "max_length":80,                    # max length for both source and target           
    "beam_max_length":80,               # should be the same as max_length
    "beam_size":5,                      # beam width         
    "gradient_accumulation_steps":1,    # number of updates steps to accumulate before performing a backward/update pass
}


config_inference = {
    # General setting
    "n_cpu": None,                      # will be filled automatically
    "seed": 1234,
    "n_gpu": 1,                         
    
    # ***** Setting to generate predictions from samples in a dataset  ***** 
    ## environment setting
    "output_path": f"{cwd}/output/roberta-bs8-chen-4321/roberta-14-00-31--09-01-2022",
    "checkpoint_path": f"{cwd}/output/roberta-bs8-chen-4321/roberta-14-00-31--09-01-2022/checkpoint/pytorch_model_roberta-14-00-31--09-01-2022.bin",
    "test_gold_datapath":f"{os.path.join(cwd, 'dataset/ready-test-clean/test_gold_clean.pkl')}",
    "test_intel_datapath":f"{os.path.join(cwd, 'dataset/ready-test-clean/test_intel_clean.pkl')}",
    # "test_intel_datapath": None,
    
    ## model setting 
    "model_type": "roberta",            # specify model type
    "load_from_repo": False,            # do not change, because we load from checkpoint

    ## inference setting
    "data_num": -1,                     # -1 to include all data, otherwise specify the number 
    "inference_batch_size": 8,              
    "beam_size":10,                     # beam width
    "top_k_inference": 10,              # save the top-k result of the inference result
    "beam_max_length":80,               # max length in the beam search
    "max_length":80,                    # max target sequence length

    #  ***** Setting to compute BLEU score and MRR ***** 
    "result_path": f"{cwd}/output/LAM/LAM-09-04-49--04-01-2022-noisy/results",                  # path to inference results
    "max_order_ngrams":4,                                                                       # setting of the BLEU n-grams
    "top_k_mrr":1,                                                                              # setting k value of MRR@k
    "smooth": True,                                                                             # set to True to use smoothing when computing BLEU
    "is_LAM": True,                                                                             # set to true if you evaluate the inference results generated by LAM
    "do_gold": False,                                                                           # set to True if you want compute the metrics for Gold15 or Test+Field
    "do_intel": False,                                                                          # set to True if you want compute the metrics for Noisy15
    "do_individual_acc": True                                                                   # set to True if you want to compute the individual accuracy
}